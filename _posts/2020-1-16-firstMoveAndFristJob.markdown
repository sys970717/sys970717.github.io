---
layout: post
title:  "첫 이직 그리고 첫 업무"
date:   2020-01-16 16:27:00 +0900
categories: [spring]
---
>12월 9일에 두번째 회사에 입사했습니다!!!!! Node 실무의 첫 경험.. 열심히 해보기로 하고 입사를 진행했죠!!
>입사하기 전에 node 와 typescript 를 이용해 백엔드를 개발하는 사이드 프로젝트를 진행했고, 아직도 진행중입니다 ( ~~입사하고 나니 갑자기 열정이 후두두둑 떨어진건 안비밀~~ )

 입사하고 1개월 정도 지나 업무를 처음으로 맡아서 했습니다!!!! (짝짝짝짝) 처음으로 맡게 된 업무는 광고사들 레포트 데이터를 웹에서 스크래핑하는 작업이었습니다. 이것을 하게 되면서 새롭게 알게 된 점, 깨달은 점 등을 정리해서 써내려가도록 하겠습니다.
# 엑셀 작업 노가다에서 크롤링으로 넘어가자!
 광고로 수익을 내기 때문에 광고사 레포트 페이지에서 확인하고 정리해야만 했었다. 지금까지는 사람이 일일이 확인하며 엑셀에 데이터를 작성했었지만 정말 단순 노가다이고 시간이 많이 든다고 한다. 이걸 한 번에 내부 CMS에서 확인하고자 하여 CTO 님이 자동화 하려고 했으나 시간이 없어서 못하고 있었다고 하셨습니다. 그렇게 저에게 떨어진 이 업무..! 
 `처음엔 단순히 request를 해서 가져오기만 하면 되고 header 세팅만 맞추면 되겠네!` 하면서 시작했는데 SPA로 작성된 페이지들을 만나고 와장창 무너졌습니다!!!!!!!!! ( 격한 분노 )

## 이제부터 시작되는 `Real` 노가다
 스크래핑, 크롤링 작업을 개발해보신 분은 아실 겁니다. **상당한 노가다** 라는 것을 말입니다.
하지만 그래도 나름 `처음 맡은 직무인데 열심히 해야지` 싶어서 Node랑 request 모듈을 사용해 총 11개 사이트를 해야하는데 처음 4개가 빨리 끝나서 `"어 3주면 좀 길겠는데??"` 했었다가 크게 한방 데인 이야기를 이제부터 풀어나갑니다

>SPA, CSR 너란 놈은 진짜...

 처음 SPA로 짜여진 웹을 맞닥뜨렸던 이 때 request로 헤더 값만 잘 맞춰넣어주면 되겠지 싶었습니다. 하지만 한 사이트에선 로그인 할 때 클라이언트에서 계정 정보를 암호화해서 서버에 전송하는 모습을 보고 **_처음은 한숨, 다음은 집착_마지막은 고뇌와 절망**을 했습니다. `내가 저 소스코드 찾고만다.` 라는 굳은 의지를 가지고 소스코드를 열었더니!! 전부 단축, 치환 되어있었습니다. 보면서 `'아 이건 진짜 아닌것 같다'` 하고 잠시 머리 식힐 겸 다른 사이트를 열었더니 그곳의 `Request` 패킷은 정말 단순해서 `'금방하고 다시 저거 해야겠다'` 라고 마음먹고 시작했습니다. 
> 의지꺾는 2차적 시련 ( 왜 내게 이런 시련이... )

 그 페이지와는 정말 request 모듈 로만 해보려하다 하루가 지나가더군요. ~~Request모듈 ssi...~~
하루를 지내보고 나니 알겠더라구요 이건 진짜 아닌 것 같다.. 라는 것을 말입니다.
쿠키가 분명 똑같은데 웹 페이지에서는 `response code 200`이 나오지만 `request`를 해보면 200이 나오지 않는.. 신기한 일들... (대략 난감)
> 이제 알게되는 테스팅 도구들의 진가

 단순히 테스트는 request 부하 테스트만 진행해봤지... 프론트 작업에 대한 테스팅 도구들은 본 적도 써본적도 없었는데 리액트 페이지 크롤링하기 관련한 검색을 통해 알게된 puppeteer 라이브러리!!
 > puppeteer?? 넌 뭐냐??

__puppeteer__ 라이브러리는 사실 프론트엔드 테스팅을 목적으로 나온 라이브러리? 툴? 입니다. 비슷한 걸로는 selenium 을 예로 들 수 있을 것 같아요!! ( 다른 건 잘 몰라서.. )
처음에 puppeteer 라이브러리를 npm 페이지에서 보고 해당 라이브러리가 경량화 된 것 `(headless chromium 이 빠짐)` 이 있다고해서 다운받았다가 반나절 고생하고 그냥 puppeteer 를 받았습니다. 설치는 ```javascript npm install puppeteer --unsafe-pem ``` 이걸로 했습니다. 자세한 방법은 구글링하시기를... ~~이거까지 적기는 길어지니까..~~
> 한결 편해진 스크래핑 ( 안도의 한숨시기 )

정말 puppeteer 를 알고나서 다시 request 로 작업하는게 너무 귀찮게 느껴졌습니다. 왜냐면 잘 쓰면 정말 편하게 쓸 수 있었고 CSR이든, API든 그런건 신경쓰지 않고 작업을 진행할 수 있었기 때문에 편했습니다. 그렇게 작업을 다 마치고 나니 개발 마감까지 시간을 잘 맞출 수 있었습니다. `(중간중간 puppeteer 로 삽 엄청 팠고... 이미 스크래핑을 완료한 한 사이트가 리뉴얼을 해버려서 다시 맞추느라 시간을 거의 다 보냈네요...)`

> 마지막 서버 반영, 왜 날 괴롭히니 AWS야...

일반적인 리눅스라고 생각하고 덤볐다가 2일동안 puppeteer에게 데였습니다. 일반적인 리눅스면 잘 될거라고 생각하고 실행 했는데 *OMG...* 
  배포는 `codePipeline` 이고 그래서 별로 신경쓰지 않았는데 `puppeteer, headless chrome` 을 설치하지 못하는 크나큰 상황에 구글링 해가며 개발서버에 `yum.. yum..`  했는데 반영엔 어려움을 겪었습니다. ~~( 사실 다 권한 문제인건 쉿쉿 )~~
```bash
$ curl https://intoli.com/install-google-chrome.sh | bash
$ vi /etc/yum.repos.d/google-chrome.repo
$ yum install google-chrome
```

위 명령어를 사용하여 크롬 드라이버를 설치 후 처리하려 했는데 ~~이번엔 `EACCES` 를 만나서 CTO 님에게 `help.. help...` 한 상황입니다.~~

2020-01-21
---
드디어 해결했습니다. chrome driver 설치했던것을 ELB (Elastic Beanstalk) 의 환경변수 속성 중 PUPPETEER_EXECUTABLE_PATH 프로퍼티 값을 google-chrome-stable 로 세팅하여 상황 종료! 이것으로 이제 운영반영을 할 수 있게 되었습니다.. 

후.. 정말 너무나도 어려웠던 AWS beanstalk + codePipeline ,, 이번 덕분에 많은 것을 경험해보고 pipeline 동작 방식도 찾아보게됐네요.. 하하핳 잘됐다 잘됐어!! 그럼 이제 글을 마치도록 하겠습니다!!
